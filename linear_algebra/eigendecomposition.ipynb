{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f85cff4-aed2-46e4-9e7e-9aa147616e79",
   "metadata": {},
   "source": [
    "# Eigenvectors and Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a020baa0-8ab2-4b29-885b-777506588319",
   "metadata": {},
   "source": [
    "An eigenvector $v$ of a matrix $A$ is a vector that is only scaled by a constant $\\lambda$ when multiplied by $A$. $\\lambda$ is the eigenvalue corresponding to the eigenvector $v$.\n",
    "\\begin{align}\n",
    "    A &\\in \\mathbb{R}^{m \\times n} \\\\\n",
    "    v &\\in \\mathbb{R}^{n} \\\\\n",
    "    \\lambda &\\in \\mathbb{R} \\\\\n",
    "    Av &= \\lambda v\n",
    "\\end{align}\n",
    "\n",
    "From the perspective that the matrix $A$ represents a linear transformation, if $v$ is an eigenvector of $A$, then all $A$ does to $v$ is scale it. $v$'s span does not change when transformed by $A$. Intuitively, eigenvectors $v_{i}$ are the vectors that are scaled by $\\lambda_{i}$ when transformed by $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324dfc1f-4469-4ee0-92e1-02aa509bb7b0",
   "metadata": {},
   "source": [
    "To compute the eigenvectors of a matrix $A$,\n",
    "\\begin{align}\n",
    "    Av &= \\lambda v \\\\\n",
    "    Av - \\lambda v &= 0 \\\\\n",
    "    \\left( A - I\\lambda \\right)v &= 0\n",
    "\\end{align}\n",
    "\n",
    "For this to have a solution, the transformation must shrink space to 0 and remove 1 dimension. This condition is expressed using the determinant:\n",
    "$$ \\mathrm{det} \\left( A -I \\lambda \\right) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020deeb-5c72-4746-9cd9-0d71916e5c1e",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db5ae4-112a-465e-b87c-8ce90b945eb7",
   "metadata": {},
   "source": [
    "Why is eigendecomposition relevant? A primary use case in machine learning is when a linear transformation $A$ needs to be applied many times:\n",
    "$$\n",
    "    A^{100}x = b\n",
    "$$\n",
    "\n",
    "In this case, finding $b$ involves 100 matrix multiplications, which is computationally very expensive. How can this be made less costly? This is where eigendecomposition can help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f1b15-9aee-42a8-afa1-6cefde0b4fd5",
   "metadata": {},
   "source": [
    "# Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8601545-a97a-46e4-8238-110a93e674ad",
   "metadata": {},
   "source": [
    "Eigendecomposition has a lot to do with [change of basis](./vectors.ipynb#Matrix-Multiplication-as-a-Change-of-Basis). Suppose we have:\n",
    "\\begin{align}\n",
    "    A &\\in \\mathbb{R}^{m \\times n} \\\\\n",
    "\\end{align}\n",
    "\n",
    "with a set of eigenvectors $v_{i}$ that spans the whole vector space $\\mathbb{R}^m$. If we construct a matrix $E$ whose columns are the eigenvectors of $A$, we have\n",
    "\n",
    "\\begin{align}\n",
    "    E &= \\begin{bmatrix} v_{1} & v_{2} & v_{3} & \\ldots v_{n}\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The significance of this matrix is that it is used for changing basis. Say we have a vector $x_{E}$ whose basis vectors are the columns of $E$.\n",
    "\n",
    "The expression $x_{B} = Ex_{E}$ can be interpreted as a change of basis to the standard basis vectors $B=\\left\\{ \\hat{i}, \\hat{j}, \\hat{k}, \\ldots \\right\\}$.\n",
    "\n",
    "The expression $Ax_{B} = AEx_{E}$ is now applying the linear transformation $A$.\n",
    "\n",
    "Finally, the expression $E^{-1}Ax_{B} = E^{-1}AEx_{E}$ changes the basis of the transformed vector back to $E$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6934dd4-3eda-42df-88c4-35a5ca01c143",
   "metadata": {},
   "source": [
    "In effect, we are taking a vector $x_{E}$, whose basis is initially $E$, changing its basis to $B$, applying the transformation $A$ and changing its basis back to $E$.\n",
    "\n",
    "Now, it turns out that the expression $E^{-1}AE$ is a [diagonal matrix](./matrices.ipynb#Diagonal-Matrices) $D =  \\mathrm{diag}\\left(\\lambda\\right)$, whose diagonal elements are the eigenvalues of $A$. To interpret this, applying the transformation $A$ to a vector $x_{B}$ in basis $B$ is equivalent to scaling the same vector $x_{E}$ in basis $E$ by the matrix $D$ whose elements are the eigenvalues of $A$.\n",
    "\n",
    "$$ E^{-1}AE = D = \\mathrm{diag}\\left(\\lambda\\right) $$\n",
    "\n",
    "Why is this significant? Because multiplication by a diagonal matrix is computationally inexpensive relative to matrix multiplication.\n",
    "\n",
    "There's one last point - we started with a vector $x_{E}$ whose basis was already $E$. What if we have a vector $x_{B}$, whose basis is in $B$, and we want to apply the transformation $A$?\n",
    "\n",
    "Since we know that the equivalent transformation of $A$ in basis $E$ is a diagonal, we could change the basis of $x_{B}$ to $E$, apply the diagonal transformation $\\mathrm{diag}\\left(\\lambda\\right)$, and change the basis back to $B$.\n",
    "\n",
    "$$ EDE^{-1} $$\n",
    "\n",
    "This idea can also be derived from the following:\n",
    "\n",
    "\\begin{align}\n",
    "    E^{-1}AE &= D \\\\\n",
    "    EE^{-1}AE &= ED \\\\\n",
    "    IAE &= ED \\\\\n",
    "    AEE^{-1} &= EDE^{-1} \\\\\n",
    "    A &= EDE^{-1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714c205-92ea-4a8e-997d-6d0e6c899e5d",
   "metadata": {},
   "source": [
    "Putting this all together, how do does this make computing $A^{100}x$ easier?\n",
    "\n",
    "\\begin{align}\n",
    "    A^{100}x &= \\left(EDE^{-1}\\right)^{100}x \\\\\n",
    "    &= \\left(EDE^{-1}\\right)\\left(EDE^{-1}\\right)\\left(EDE^{-1}\\right)\\left(EDE^{-1}\\right)\\left(EDE^{-1}\\right)^{96}x \\\\\n",
    "    &= EDE^{-1}EDE^{-1}EDE^{-1}EDE^{-1}\\left(EDE^{-1}\\right)^{96}x \\\\\n",
    "    &= ED^{4}E^{-1}\\left(EDE^{-1}\\right)^{96}x \\\\\n",
    "    &= ED^{100}E^{-1}x\n",
    "\\end{align}\n",
    "\n",
    "Computing $D^{100}$ is relatively computationally inexpensive. It would simply be the 100th power of each diagonal element in the diagonal matrix $D = \\mathrm{diag}\\left(\\lambda\\right)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
