{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed707d2-cf68-4b4c-955c-a5acd5c25413",
   "metadata": {},
   "source": [
    "This notebook contains a set of transformations on the training set without the use of pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "8b40c232-948b-4cae-a71c-ba818ba36ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Constants\n",
    "data_file_path = \"./data/home-data-for-ml-course/train.csv\"\n",
    "test_size = 0.2\n",
    "val_size = 0.2\n",
    "random_state = 0\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Target and features\n",
    "y = df.SalePrice\n",
    "\n",
    "# All numeric without missing values\n",
    "features = list(set(df.columns) - set([\"SalePrice\"]))\n",
    "X = df[features]\n",
    "\n",
    "# Splitting\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=val_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "bfa5d5b3-6e20-4008-907b-6889ebc46291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing source code that applies the same several preprocessing steps to different\n",
    "# datasets can quickly become messy.\n",
    "\n",
    "# Suppose we want to do the following preprocessing on any one dataset:\n",
    "# - Track missing values\n",
    "# - Impute categorical features with most frequent\n",
    "# - One-hot encode all categorical features with 10 or less unique values\n",
    "# - Ordinal encode all other categorical features\n",
    "# - Impute missing values of numerical features with mean\n",
    "# - Standard-scale all numerical variables\n",
    "\n",
    "# Define some utility functions\n",
    "def columns_with_missing_values(df):\n",
    "    \"\"\"Get list of columns with missing values\"\"\"\n",
    "    missing_value_counts = df.isnull().sum()\n",
    "    return list(missing_value_counts[missing_value_counts > 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "6e9a064d-27a8-45c8-a4f8-d208b1958826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categorical and numerical column names\n",
    "num_cols = list(train_X.select_dtypes(include=[\"number\"]).columns)\n",
    "cat_cols = list(set(train_X.columns) - set(num_cols))\n",
    "\n",
    "# Track missing values\n",
    "train_X_orig = train_X.copy()\n",
    "for col in columns_with_missing_values(train_X):\n",
    "    train_X[col + \"_missing\"] = train_X[col].isnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "99275ed9-02ab-4fa9-ad77-415d7b463258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess numerical features\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Impute numerical columns with mean\n",
    "num_feat = train_X[num_cols]\n",
    "num_feat_imputed = pd.DataFrame(data=SimpleImputer().fit_transform(num_feat),\n",
    "                                columns=num_feat.columns,\n",
    "                                index=num_feat.index)\n",
    "train_X = train_X.drop(labels=num_cols, axis=1)\n",
    "train_X = pd.concat([train_X, num_feat_imputed], axis=1)\n",
    "\n",
    "# Standard-scale numerical features\n",
    "num_feat = train_X[num_cols]\n",
    "num_feat_scaled = pd.DataFrame(data=StandardScaler().fit_transform(num_feat),\n",
    "                               columns=num_feat.columns,\n",
    "                               index=num_feat.index)\n",
    "train_X = train_X.drop(labels=num_cols, axis=1)\n",
    "train_X = pd.concat([train_X, num_feat_scaled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "93d75547-a91e-4053-898b-3de4d3dc58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess categorical features\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# Categorical columns and their unique counts\n",
    "cat_feat = train_X[cat_cols]\n",
    "\n",
    "# Impute missing variables\n",
    "missing = cat_feat.isnull().sum()\n",
    "missing = list(missing[missing > 0].index)\n",
    "cat_feat_missing = cat_feat[missing]\n",
    "cat_feat_imputed = pd.DataFrame(data=SimpleImputer(strategy=\"most_frequent\").fit_transform(cat_feat_missing),\n",
    "                                     columns=cat_feat_missing.columns,\n",
    "                                     index=cat_feat_missing.index)\n",
    "train_X = train_X.drop(labels=missing, axis=1)\n",
    "train_X = pd.concat([train_X, cat_feat_imputed], axis=1)\n",
    "\n",
    "# One-hot encode only categorical variables with 10 or less distinct values\n",
    "unique_counts = cat_feat.nunique()\n",
    "cols_to_ohe = list(unique_counts[unique_counts <= 10].index)\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "cat_feat_ohe = ohe.fit_transform(cat_feat[cols_to_ohe])\n",
    "cat_feat_ohe = pd.DataFrame(data=cat_feat_ohe,\n",
    "                            columns=ohe.get_feature_names_out(),\n",
    "                            index=cat_feat.index)\n",
    "train_X = train_X.drop(labels=cols_to_ohe, axis=1)\n",
    "train_X = pd.concat([train_X, cat_feat_ohe], axis=1)\n",
    "\n",
    "# Ordinal encode the rest of the columns\n",
    "cols_to_oe = list(set(cat_cols) - set(cols_to_ohe))\n",
    "oe = OrdinalEncoder()\n",
    "cat_feat_oe = oe.fit_transform(cat_feat[cols_to_oe])\n",
    "cat_feat_oe = pd.DataFrame(data=cat_feat_oe,\n",
    "                           columns=oe.get_feature_names_out(),\n",
    "                           index=cat_feat.index)\n",
    "train_X = train_X.drop(labels=cols_to_oe, axis=1)\n",
    "train_X = pd.concat([train_X, cat_feat_oe], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a607e553-f48a-4d21-83cb-2d95016b34f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for columns that are not in the transformed training set but that are in the original\n",
    "cols_diff = list(set(train_X_orig.columns) - set(train_X.columns))\n",
    "\n",
    "# For every column not found in the transformed training set, check if there are columns\n",
    "# in the transformed training set that start with the column name. They could have been one-hot encoded.\n",
    "cols_not_found = []\n",
    "for col in cols_diff:\n",
    "    if not(train_X.columns.str.startswith(col).any()):\n",
    "        # None of the columns in the transformed training set start with the column name\n",
    "        cols_not_found.append(col)\n",
    "assert len(cols_not_found) == 0, f\"Columns in the original training data not found. {cols_not_found}\"\n",
    "\n",
    "# All columns in the original training set are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "cd668131-7120-4ee4-bddd-8f6730e8d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the transformed training set does not have missing columns\n",
    "assert len(columns_with_missing_values(train_X)) == 0, \"Transformed training set still has missing columns\"\n",
    "\n",
    "# Check that all columns with missing data in the orignial set have a \"_missing\"\n",
    "# column in the transformed training set\n",
    "prob_cols = []\n",
    "for col in columns_with_missing_values(train_X_orig):\n",
    "    if not(np.array(train_X.columns == f\"{col}_missing\").any()):\n",
    "        prob_cols.append(col)\n",
    "assert len(prob_cols) == 0, f\"Some columns with missing values were not tracked. {prob_cols}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "5cc9e46e-134e-4aa1-b967-4d1fb52fa524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all categorical variables were properly encoded\n",
    "assert len(train_X.select_dtypes(exclude=[\"number\"]).columns) == 0, \\\n",
    "    \"Transformed training set still has non-numeric types\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "ad7421b7-03d1-4afa-9c9f-b626630b806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all numerical features were standard scaled\n",
    "assert train_X[num_cols].mean().abs().between(0, 1e-6).all(), \\\n",
    "    \"Some numerical features do not have a mean close to 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a62c8575-d4ec-4aba-a16f-580fbca3bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now all this needs to be repeated again for the validation set.\n",
    "# Pipelines are a better way of packaging these transformations so that they can be applied\n",
    "# to multiple sets of data properly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
