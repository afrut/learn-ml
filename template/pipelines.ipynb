{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "8b40c232-948b-4cae-a71c-ba818ba36ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Constants\n",
    "data_file_path = \"./data/train.csv\"\n",
    "test_size = 0.2\n",
    "val_size = 0.2\n",
    "random_state = 0\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Target and features\n",
    "target = \"SalePrice\"\n",
    "y = df.SalePrice\n",
    "\n",
    "# All numeric without missing values\n",
    "features = list(set(df.columns) - set([\"SalePrice\"]))\n",
    "X = df[features]\n",
    "\n",
    "# Splitting\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=val_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "bfa5d5b3-6e20-4008-907b-6889ebc46291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing source code that applies the same several preprocessing steps to different\n",
    "# datasets can quickly become messy.\n",
    "\n",
    "# Suppose we want to do the following preprocessing on any one dataset:\n",
    "# - Track missing values\n",
    "# - Impute categorical features with most frequent\n",
    "# - One-hot encode all categorical features with 10 or less unique values\n",
    "# - Ordinal encode all other categorical features\n",
    "# - Impute missing values of numerical features with mean\n",
    "# - Standard-scale all numerical variables\n",
    "\n",
    "# Here are some custom transfromers.\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import set_config\n",
    "\n",
    "# Transformer outputs will now output DataFrames\n",
    "set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "49366279-107b-4195-8d2b-1ae57d35bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackingImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Create a column that corresponds to a feature whose values\n",
    "    indicate if a sample is missing the feature value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    @staticmethod\n",
    "    def columns_with_missing_values(df):\n",
    "        \"\"\"Get list of columns with missing values\"\"\"\n",
    "        missing_value_counts = df.isnull().sum()\n",
    "        return list(missing_value_counts[missing_value_counts > 0].index)\n",
    "    def fit(self, X, y=None):\n",
    "        self.columns = self.columns_with_missing_values(X)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for c in self.columns:\n",
    "            X[c + \"_missing\"] = X[c].isnull().astype(int)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "6e9a064d-27a8-45c8-a4f8-d208b1958826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_numeric_columns(df):\n",
    "    \"\"\"Get non-numeric columns\"\"\"\n",
    "    return list(df.select_dtypes(exclude=[\"number\"]).columns)\n",
    "\n",
    "def get_low_cardinality_columns(df):\n",
    "    \"\"\"Get list of categorical columns 10 or less unique values\"\"\"\n",
    "    cols = get_non_numeric_columns(df)\n",
    "    cols_unq_cnts = df[cols].nunique()\n",
    "    low_card_cols = list(cols_unq_cnts[cols_unq_cnts <= 10].index)\n",
    "    return low_card_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "1c455315-e196-45be-b849-71430cae6d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categorical and numerical columns\n",
    "num_cols = list(train_X.select_dtypes(include=[\"number\"]).columns)\n",
    "cat_cols = list(set(train_X.columns) - set(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "8e54e703-3eb3-435a-8aea-3e77c6ec4ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for categorical features\n",
    "low_card_cols = get_low_cardinality_columns(train_X)\n",
    "cols_to_oe = list(set(cat_cols) - set(low_card_cols))\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    (\"impute_most_frequent\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encode\", ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"one_hot_encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), low_card_cols),\n",
    "            (\"ordinal_encoder\", OrdinalEncoder(dtype=np.int64,\n",
    "                                               handle_unknown=\"use_encoded_value\",\n",
    "                                               unknown_value = -1), cols_to_oe)\n",
    "        ]\n",
    "        , remainder=\"passthrough\", verbose_feature_names_out = False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "3b66179c-c2bb-4713-9c09-7d2e2e8830ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for numerical variables\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    (\"impute_mean\", SimpleImputer()),\n",
    "    (\"standard_scale\", StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "5e321001-cabf-4cac-975c-d14ae798698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble preprocessing steps\n",
    "preprocessing = Pipeline(steps=[\n",
    "    (\"track_missing\", TrackingImputer()),\n",
    "    (\"numerical_categorical_transformer\", ColumnTransformer(transformers=[\n",
    "            (\"transform_categorical\", cat_pipeline, cat_cols),\n",
    "            (\"transform_numerical\", num_pipeline, num_cols)\n",
    "        ],\n",
    "        remainder=\"passthrough\", verbose_feature_names_out = False)\n",
    "    )\n",
    "])\n",
    "\n",
    "# Assemble final pipeline, fit and transform\n",
    "mdl = Pipeline(steps=[\n",
    "    (\"preprocessing\", preprocessing),\n",
    "    (\"model\", RandomForestRegressor(random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "7e914247-7701-4cdd-953c-77dae61560b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17223.706111111114\n"
     ]
    }
   ],
   "source": [
    "# Score on the validation set\n",
    "mdl.fit(train_X, train_y)\n",
    "print(mean_absolute_error(val_y, mdl.predict(val_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa4ef6a-f92b-4fe8-9a18-b809eb308fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the transformation\n",
    "\n",
    "# Compare shape of original vs transformed\n",
    "print(\"Original training shape = \" + str(train_X.shape))\n",
    "print(\"Transformed training shape = \" + str(train_X_transformed.shape))\n",
    "\n",
    "# Check that there are no missing values\n",
    "missing = train_X_transformed.isnull().any()\n",
    "cols_with_missing = missing[missing].index\n",
    "assert len(missing[missing]) == 0, f\"Some columns have missing values: {cols_with_missing}\"\n",
    "\n",
    "# Check that there are now no categorical types\n",
    "cols = train_X_transformed.select_dtypes(exclude=[\"number\"]).columns\n",
    "assert len(cols) == 0, f\"Some columns are not properly encoded: {cols}\"\n",
    "\n",
    "# Check that there are _missing columns\n",
    "idx = train_X_transformed.columns.str.endswith(\"_missing\")\n",
    "assert len(idx[idx]) > 0, \"Track missing columns _missing not found\"\n",
    "\n",
    "# Check that the original low cardinality columns are now missing using the intersection of sets\n",
    "assert len(set(train_X_transformed.columns) & set(low_card_cols)) == 0, \\\n",
    "    \"Some low cardinality columns are still found in the final set. They should have been one-hot encoded\"\n",
    "\n",
    "# Check that all low cardinality columns have new columns that start with the original column name\n",
    "assert np.array([train_X_transformed.columns.str.startswith(col).any() for col in low_card_cols]).all(), \\\n",
    "    \"Some low cardinality columns were not one-hot encoded properly\"\n",
    "\n",
    "# Check that all these columns are either 0 or 1\n",
    "list_of_lists = [list(train_X_transformed.columns[train_X_transformed.columns.str.startswith(col)]) \\\n",
    "    for col in low_card_cols]\n",
    "low_card_cols_transformed = [col for ls in list_of_lists for col in ls if not col.endswith(\"_missing\")]\n",
    "assert train_X_transformed[low_card_cols_transformed].isin([0, 1]).all().all(), \\\n",
    "    \"Some a one-hot encoded low cardinality columns have a value different from 0 or 1\"\n",
    "\n",
    "# Check that all ordinal-encoded features are ints\n",
    "assert (train_X_transformed[cols_to_oe].dtypes == np.int64).all(), \\\n",
    "    \"Some ordinal encoded columns are not ints\"\n",
    "\n",
    "# Check that all ordinal-encoded features have more than 1 unique value\n",
    "assert (train_X_transformed[cols_to_oe].nunique() > 1).all(), \\\n",
    "    \"Some ordinal encoded columns have less than 1 unique value\"\n",
    "    \n",
    "# Check that all numerical columns have mean of about 0\n",
    "assert train_X_transformed[num_cols].describe().loc[\"mean\",:].between(-1e-6, 1e-6).all(), \\\n",
    "    \"Some numerical features were not mean-centered\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
