{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to be an informal reference for point estimation aimed at engineers. All content here is taken from Montgomery and Runger - Applied Statistics and Probability for Engineers 7ed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Inferential Statistics</h1>\n",
    "\n",
    "The next sections form the foundations of inferential statistics. Inferential statistics aims to draw conclusions about a population by examining a sample drawn from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Point Estimation</h1>\n",
    "\n",
    "Many situations arise where an estimate of a generally unknown parameter is needed. Take the following examples:\n",
    "<ul>\n",
    "    <li>the average weight of a bag of chips produced at a factor</li>\n",
    "    <li>the average height of a population of a country</li>\n",
    "    <li>the standard deviation of temperature of a region</li>\n",
    "</ul>\n",
    "It is quite often infeasible to measure the value of a <b>population parameter $\\theta$</b>. For example, it would be very expensive to measure the weight of every bag of chips that was produced from a factory!\n",
    "\n",
    "Instead, a <b> random sample</b> is drawn and measurements taken. A bag of 100 chips can be weighed and their average weight computed. In this case, the sample mean $\\overline{x}$ is called a <b>point estimator $\\hat{\\Theta}$</b> and its value is called a <b>point estimate $\\hat{\\theta}$</b> of the population parameter, which is the mean weight of all bags. A point estimator can also be thought of as a function performed on the sample to generate an estimate. Any function of the observations of a random sample is known as a <b>statistic</b>. The point estimator that is the sample mean is one such example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sampling Distributions</h1>\n",
    "\n",
    "The sample mean is also a random variable. If 3 samples are taken (a total of 300 bags of chips), 3 sample means can be computed. The 3 sample means will be different leading to the idea that the sample mean has a <b>sampling distribution</b>. Thus, point estimates can be thought of as having a probability distribution called the sampling distribution. Intuitively, this makes sense since every sample taken from a population will be slightly different. If observations for every are slightly different, then it follows that a function (which a point estimator is) of the slightly different observations will also yield different values.\n",
    "\n",
    "The <b>Central Limit Theorem</b> states that the distribution of sample means $\\overline{x}$ over many, many samples tends to approach a standard normal distribution regardles of the population parameter's $\\theta$ probability distribution. Mathematically, the Central Limit Theorem states that the distribution of $Z$ approaches the standard normal distribution as the number of samples approaches infinity.\n",
    "<font size=\"4\">\n",
    "    $$Z=\\frac{\\overline{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}$$<br>\n",
    "    $$\\overline{X}\\rightarrow\\mu$$<br>\n",
    "    $$\\sigma^2_{\\overline{X}}\\rightarrow\\frac{\\sigma^2}{n}$$<br>\n",
    "    $$\\sigma_{\\overline{X}}\\rightarrow\\frac{\\sigma}{\\sqrt{n}}$$<br>\n",
    "    $$n\\rightarrow\\infty$$\n",
    "</font>\n",
    "\n",
    "Similarly, for a random variable that is the difference between between two sample means $\\overline{X}_1$ and $\\overline{X}_2$, the sampling distribution of the following $Z$ is approximately standard normal:<br><br>\n",
    "<font size=\"4\">\n",
    "    $$Z=\\frac{\\overline{X}_1 - \\overline{X}_2 - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}}}$$<br>\n",
    "    $$\\overline{X}_1 - \\overline{X}_2 \\rightarrow \\mu_1 - \\mu_2$$<br>\n",
    "    $$\\sigma^2_{\\overline{X}_1 - \\overline{X}_2} \\rightarrow \\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}$$<br>\n",
    "    $$\\sigma_{\\overline{X}_1 - \\overline{X}_2} \\rightarrow \\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}}$$<br>\n",
    "    $$n\\rightarrow\\infty$$\n",
    "</font><br>\n",
    "\n",
    "Where\n",
    "<ul>\n",
    "    <li>$\\overline{X}_1$ is the sample mean from the population 1</li><br>\n",
    "    <li>$\\overline{X}_2$ is the sample mean from the population 2</li><br>\n",
    "    <li>$\\mu_1$ is the mean of population 1</li><br>\n",
    "    <li>$\\mu_2$ is the mean of population 2</li><br>\n",
    "    <li>$\\sigma^2_1$ is the variance of population 1</li><br>\n",
    "    <li>$\\sigma^2_2$ is the variance of population 2</li><br>\n",
    "    <li>$n_1$ is the sample size of the sample from population 1</li><br>\n",
    "    <li>$n_2$ is the sample size of the sample from population 2</li><br>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>General Concepts of Point Estimation</h1>\n",
    "\n",
    "With the idea of a sampling distribution comes the idea of the expected value of a point estimator. The expected value of a point estimator is the sum of the products of the point estimates and their corresponding probabilities as given by the sampling distribution. How does this value differ from the population parameter? The difference between the expected value of a point estimator and the value of the population parameter is called <b>bias</b>:<br><br>\n",
    "\n",
    "<font size=\"4\">\n",
    "    $$\\mathrm{bias} = E(\\hat{\\Theta})-\\theta$$\n",
    "</font>\n",
    "\n",
    "A point estimator is <b>unbiased</b> when $E(\\hat{\\Theta})=\\theta$.<br><br>\n",
    "\n",
    "What about the variance of a point estimator's sampling distribution? Well, a smaller variance implies a higher chance of observing a point estimate that is close to the expected value of the point estimator. This leads to the concept of the <b>minimum variance unbiased estimator</b> MVUE. In other words, the MVUE is the best estimator because it is centered around the population parameter (unbiased) and there is a high chance of obtaining a point estimate that is close to the expected value (minimum variance). The sample mean $\\overline{X}$ is the MVUE of the population mean $\\mu$.<br>\n",
    "\n",
    "The <b>standard error</b> of a point estimator can be used communicate the precision of its point estimates. $\\mathrm{SE}(\\hat{\\Theta})$ is the operation of calculating the standard deviation of the sampling distribution of that statistic. Mathematically:<br><br>\n",
    "\n",
    "<font size=\"4\">\n",
    "    $$\\mathrm{SE}(\\hat{\\Theta})=\\sqrt{V(\\hat{\\Theta})}$$<br>\n",
    "    $$\\mathrm{SE}(\\overline{X})=\\frac{\\sigma}{\\sqrt{n}}$$\n",
    "</font><br>\n",
    "\n",
    "The <b>estimated standard error</b> of a point estimate is used when estimates are used in place of generally unknown parameters. For example, with a large sample size $n$, the variance and standard deviation of the sampling distribution of the sample mean $\\overline{X}$ are:<br><br>\n",
    "\n",
    "<font size=\"4\">\n",
    "    $$\\sigma^2_{\\overline{X}}=\\frac{\\sigma^2}{n}$$<br>\n",
    "    $$\\mathrm{SE}(\\overline{X})=\\sigma_{\\overline{X}}=\\frac{\\sigma}{\\sqrt{n}}$$\n",
    "</font><br>\n",
    "\n",
    "But in general, the variance $\\sigma^2$ of the population distribution. What can be done is to use the sample variance $s^2$ or sample standard deviation $s$ to calculate an estimate for the standard error of $\\overline{X}$:<br><br>\n",
    "\n",
    "<font size=\"4\">\n",
    "    $$\\hat{\\sigma}_\\overline{X}=\\frac{s}{\\sqrt{n}}$$\n",
    "</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Bootstrapping</h1>\n",
    "\n",
    "For the sample mean point estimator $\\overline{X}$, the Central Limit Theorem was used to derive the result that $\\sigma_{\\overline{X}}=\\frac{\\sigma}{n}$ (derivation not included in notes). However, some point estimators $\\hat{\\Theta}$ have complicated forms and derivation of an expression for its standard error is difficult, if not impossible. The <b>bootstrap</b> technique can be used to estimate the standard error of the point estimator $\\mathrm{SE}(\\hat{\\Theta})$.\n",
    "\n",
    "Suppose that a random sample from the population is present to characterize the population parameter $\\theta$. Suppose further that the <b>probability density function of the population is known</b>. With the use of a computer, bootstrapping proceeds as follows:<br>\n",
    "<ul>\n",
    "    <li>Using the sample, calculate the sample point estimate $\\hat{\\theta}$ of the population parameter $\\theta$.</li>\n",
    "    <li>Use the known pdf of the population to generate samples. These are known as bootstrap samples.</li>\n",
    "    <li>Compute the bootstrap estimate $\\hat{\\theta}_B$ of the population parameter $\\theta$.</li>\n",
    "    <li>Repeat generating samples and calculating bootstrap estimates $\\hat{\\theta}_B$ $n_B$ times.</li>\n",
    "    <li>The $n_B$ bootstrap estimates $\\{\\hat{\\theta_1}_B, \\hat{\\theta_2}_B, \\hat{\\theta_3}_B, \\ldots, \\hat{\\theta_{n_B}}_B\\}$of $\\theta$ can be treated as the sampling distribution of the point estimate $\\hat{\\theta}$ from the original sample.</li>\n",
    "    <li>The standard deviation of the sampling distribution formed by the bootstrap estimates is an estimate of the standard error of the point estimate $\\hat{\\theta}$:</li>\n",
    "</ul>\n",
    "<font size =\"4\">\n",
    "    $$\\overline{\\theta}_B=\\frac{1}{n_B}\\sum_{i=1}^{n_B}{\\hat{\\theta_i}_B}$$<br>\n",
    "    $$\\mathrm{SE}(\\hat{\\theta})=\\sqrt{\\frac{1}{n_B - 1} \\sum_{i=1}^{n_B}{(\\hat{\\theta_i}_B - \\overline{\\theta}_B})^2}$$\n",
    "</font>\n",
    "\n",
    "If the <b>probability density function of the population is unknown</b>, which is probably more often the case, the sample can be treated as the population. Samples can then be drawn with replacement to build a set of boot strap estimates $\\{\\hat{\\theta_1}_B, \\hat{\\theta_2}_B, \\hat{\\theta_3}_B, \\ldots, \\hat{\\theta_{n_B}}_B\\}$of $\\theta$. This set of bootstrap estimates can then be treated as the sampling distribution of $\\hat{\\theta}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
