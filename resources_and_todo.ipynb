{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee68b56-de65-40f9-a829-a33b68b11442",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7edb8b9",
   "metadata": {},
   "source": [
    "\n",
    "- Goodfellow, Deep learning, Chapter 2, Linear Algebra\n",
    "  - $x^{T}Ax$ as it relates to definiteness (positive, semi-positive, etc)\n",
    "  - Singular Value Decomposition (SVD)\n",
    "  - Moore-Penrose Pseudoinverse\n",
    "  - Vectors and projection\n",
    "  - Trace operator\n",
    "  - [principal component analysis and dimension reduction](https://www.youtube.com/watch?v=HMOI_lkzW08&t=27s&pp=ygUdcHJpbmNpcGFsIGNvbXBvbmVudHMgYW5hbHlzaXM%3D)\n",
    "- Goodfellow, Deep learning, Chapter 3, Probability and Information Theory\n",
    "- Goodfellow, Deep learning, Chapter 4, Numerical Methods\n",
    "- Goodfellow, Deep learning, Chapter 5, Machine Learning Basics\n",
    "- Statistics\n",
    "  - covariance matrix\n",
    "  - graphical models/structured probabilistic models and probability distribution factorizations\n",
    "  - condition number\n",
    "- Information Theory\n",
    "  - information theory Cover and Thomas (2006) or MacKay (2003)\n",
    "  - entropy and distributions\n",
    "- Vector calculus\n",
    "  - Gradient\n",
    "  - Jacobian\n",
    "- lightgbm\n",
    "- catboost\n",
    "- https://www.neuraxio.com/blogs/news/whats-wrong-with-scikit-learn-pipelines\n",
    "- Endogeneity in model evaluation\n",
    "- Causal Inference\n",
    "- Interpretability and Explainability\n",
    "- SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae068bb-58e8-4d19-80ad-d153a33644bd",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99bff16-48db-4f1d-a664-54ddbbdee66d",
   "metadata": {},
   "source": [
    "- [Montgomery, Applied Statistics and Probability for Engineers 7ed](https://www.wiley.com/en-us/Applied+Statistics+and+Probability+for+Engineers%2C+7th+Edition-p-9781119400363)\n",
    "- [Goodfellow, Deep Learning](https://www.deeplearningbook.org/)\n",
    "- [John Krohn's Youtube Channel](https://www.youtube.com/@JonKrohnLearns/videos)\n",
    "  - [Probability for Machine Learning](https://www.youtube.com/playlist?list=PLRDl2inPrWQWwJ1mh4tCUxlLfZ76C1zge) \n",
    "  - [Calculus for Machine Learning](https://www.youtube.com/playlist?list=PLRDl2inPrWQVu2OvnTvtkRpJ-wz-URMJx)\n",
    "  - [Linear Algebra for Machine Learning](https://www.youtube.com/playlist?list=PLRDl2inPrWQW1QSWhBU0ki-jq_uElkh2a)\n",
    "  - [Github](https://github.com/jonkrohn/ML-foundations)\n",
    "- [Principal Component Analysis](https://www.youtube.com/watch?v=HMOI_lkzW08&t=27s&pp=ygUdcHJpbmNpcGFsIGNvbXBvbmVudHMgYW5hbHlzaXM%3D)\n",
    "- [ritvikmath Data Science Concepts](https://www.youtube.com/playlist?list=PLvcbYUQ5t0UH2MS_B6maLNJhK0jNyPJUY)\n",
    "- [oninestatbook](https://onlinestatbook.com)\n",
    "- [PennState Stat415](https://online.stat.psu.edu/stat415/)\n",
    "- [Kaggle Courses](https://www.kaggle.com/search?q=+in%3Acourses)\n",
    "- [Introduction to Statistical Learning Videos](https://www.youtube.com/watch?v=5N9V07EIfIg&list=PLOg0ngHtcqbPTlZzRHA2ocQZqB1D_qZ5V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93979984",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b59971",
   "metadata": {},
   "source": [
    "- Preprocessing\n",
    "  - Scaling\n",
    "  - Handling Outliers\n",
    "- Feature Engineering\n",
    "  - Mutual Information\n",
    "  - Dimensionality Reduction\n",
    "  - PCA\n",
    "  - Curse of Dimensionality\n",
    "- Information Theory\n",
    "  - Entropy\n",
    "- Overfitting vs Underfitting\n",
    "- Different Types of ML Models\n",
    "  - Decision Trees\n",
    "  - Random Forest\n",
    "  - XGBoost\n",
    "  - LightGBM\n",
    "- Ensemble Methods\n",
    "  - Gradient Boosting\n",
    "- Missing Values\n",
    "  - Drop columns\n",
    "  - Impute with Mean Value\n",
    "  - Add a new column/feature indicating the missing value\n",
    "- Categorical Variables\n",
    "  - Drop\n",
    "  - Ordinal Encoding\n",
    "  - One-Hot Encoding\n",
    "    - Find ways of handling features with high cardinality\n",
    "- Hyperparameter tuning\n",
    "  - Parameter search, grid search\n",
    "- Kernel Approximation\n",
    "- Kernel Density/Kernel Density Plot\n",
    "- Misc\n",
    "  - Kaggle Deep Learning Free GPU Access\n",
    "  - Google Colab\n",
    "- sklearn\n",
    "  - sklearn.metrics.mean_absolute_error\n",
    "  - sklearn.impute.SimpleImputer\n",
    "  - sklearn.model_selection.cross_val_score\n",
    "  - GridSearchCV()\n",
    "  - sklearn.pipeline.make_pipeline\n",
    "  - sklearn.feature_selection.mutual_info_regression\n",
    "- pandas\n",
    "  - pd.get_dummies\n",
    "  - pd.factorize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0241d33f",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25886378",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ebbb0b",
   "metadata": {},
   "source": [
    "### Understand Spread and Disperson\n",
    "- Does the feature vary? A feature with little variation or that is almost\n",
    "  constant contributes little to explaining the variability in the target.\n",
    "- Is the feature normal?\n",
    "    - Standard scaling works better on normally distributed features.\n",
    "    - Models that assume normality: Gaussian Naive Bayes, Linear Discriminant\n",
    "    Analysis, Quadratic discriminant analysis\n",
    "    - PCA benefits from normality if you intend to use the principal components\n",
    "      as uncorrelated variables\n",
    "    - Affects imputation strategy\n",
    "        - Mean imputation for missing values\n",
    "        - Median for highly skewed distributions\n",
    "        - KNN imputation or predicting missing values based on other features\n",
    "            - Check if imputation has distorted the original distribution\n",
    "    - Tests\n",
    "      - Shapiro-Wilk: Computes correlation between samples and normal scores.\n",
    "        Good for <5000 samples. A W closer to 1 indicates normality.\n",
    "      - Kolmogorov-Smirnov: Compares the cumulative distribution function of the\n",
    "        sample data with the CDF of the normal distribution. It measures the\n",
    "        maximum vertical distance between these two curves. Needs true\n",
    "        population mean and variance.\n",
    "      - Lilliefors Test: an improvement on Kolmogorov-Smirnov. Uses corrected\n",
    "        critical values to account for uknown population mean and variance.\n",
    "      - Anderson-Darling: Similar to Kolmogorov-Smirnov but gives more weight to\n",
    "        the tails of the distribution.\n",
    "      - Jarque-Bera: Based on skewness and kurtosis. Recommended for larger\n",
    "        sample sizes >2000.\n",
    "      - These tests have low power at low sample sizes and become too sensitive\n",
    "        at high sample sizes. Visual plots are still useful.\n",
    "- Check for homoscedacity - is the variance constant? \n",
    "- Are there gaps, clusters or spikes?\n",
    "- Do the min/max make sense?\n",
    "- Look at the distribution of the target conditioned on different classes or\n",
    "  ranges of the feature. Are there interactions? Are the means and variances of\n",
    "  these distributions the same? Are the distributions similar or different?\n",
    "    - Mann-Whitney U test\n",
    "    - q-q plots\n",
    "- If the distribution is multi-modal (many peaks), each peak could represent a\n",
    "  sub-group in the data. Include the underlying categorical variable if you can\n",
    "  find it.\n",
    "    - If there is no existing categorical variable, a new one can be created if\n",
    "      the subgroups can be defined. See K-Means Clustering and Gaussian Mixutre\n",
    "      Models.\n",
    "    - Another possibility is to decompose the multi-modal feature into multiple features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f4ebfa",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "- Do the outleirs make sense?\n",
    "    - If they are due to errors, drop them.\n",
    "    - If not, try Winsorization/capping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d84302",
   "metadata": {},
   "source": [
    "### Skewness:\n",
    "- If the feature is positively skewed, try the following transformations:\n",
    "    - log transformation, square root, Box-Cox\n",
    "- Yeo-Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626ed58",
   "metadata": {},
   "source": [
    "### Predictive Power\n",
    "- Signal to Noise ratio (SNR); how much of a feature contributes to predicting the\n",
    "  target and how much is noise?\n",
    "- A core strategy is to quantify how much of the variation in the target\n",
    "  variable is explained by variation in a feature?\n",
    "    - Correlation captures linear relationships.\n",
    "    - R<sup>2</sup>?\n",
    "    - Mutual Information\n",
    "    - ANOVA bins the continuous feature and checks if the mean of the target\n",
    "      variable differs across these bins.\n",
    "    - Tree-based feature importance\n",
    "    - Permutation importance. Train a model. Then randomly arrange the values of\n",
    "      a feature across a sample. Then train the same model again. Did the new\n",
    "      model perform worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25af1da",
   "metadata": {},
   "source": [
    "multicollinearity among features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
